{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5anXI-PmULE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRu6NYfemULG"
      },
      "outputs": [],
      "source": [
        "# 10.1 Identifying Missing Data\n",
        "# We will use the orders table from Super Store for these tasks\n",
        "orders = pd.read_csv('./datasets/orders.csv')\n",
        "\n",
        "\n",
        "# A. Start with some exploratory analysis methods to inspect the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIHlZBBcmULH"
      },
      "outputs": [],
      "source": [
        "# B. Which column has the most missing data? Sort the columns by sum of null values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBtWlkqlmULI"
      },
      "outputs": [],
      "source": [
        "# C. Looks like postal_code is our biggest problem, along with region_id\n",
        "# Use a combination of filtering, isnull, and sum to count how many rows are missing both columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkmo70kumULJ"
      },
      "outputs": [],
      "source": [
        "# D. Let's drop the region_id nulls from the dataframe before proceeding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIQ2m-okmULK"
      },
      "outputs": [],
      "source": [
        "# E. It's the dream scenario! The IT team confirms all missing postal_code values should be 10001.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33YJUZ0JmULK"
      },
      "outputs": [],
      "source": [
        "# 10.2 Cleaning Our Data\n",
        "# A. Write a profit_margin function that accepts a row of data, which is a dictionary\n",
        "#    It should return the result of dividing the profit column by the sales column (i.e. profit/sales)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp3Y3kZ3mULK"
      },
      "outputs": [],
      "source": [
        "# B. Create a new column in the orders dataframe called 'profit_margin' by applying the profit margin function row-by-row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy3NkFYwmULL"
      },
      "outputs": [],
      "source": [
        "# C. Use the same process to create a new column called margin_category\n",
        "# If the profit_margin is less than 0, the margin_category should be \"unprofitable\"\n",
        "# If the profit_margin is 0, the margin_category should be \"break even\"\n",
        "# If the profit_margin is above 0, the margin_category should \"profitable\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAnxrNffmULM"
      },
      "outputs": [],
      "source": [
        "# D. How many of our orders were unprofitable?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBnIsZpemULM"
      },
      "outputs": [],
      "source": [
        "# 10.3 GroupBy Insights\n",
        "# Segment the following data and explore aggregate values to answer the following questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "BGYD6k4emULN"
      },
      "outputs": [],
      "source": [
        "# A. Which discount results in the highest mean order quantity?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KTey4J5cmULN"
      },
      "outputs": [],
      "source": [
        "# B. Which product has the highest mean price discount applied?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egOto1nQmULO"
      },
      "outputs": [],
      "source": [
        "# 10.4 Joining DataFrames\n",
        "# The below example joins our first two dataframes by their shared column, Symbol\n",
        "openprice = pd.DataFrame({'Symbol': ['AAPL', 'DHR', 'DAL', 'AMZN'], 'OpenPrice': [217.51, 96.54, 51.45, 1703.34]})\n",
        "wkhigh = pd.DataFrame({'Symbol': ['DAL', 'AMZN', 'AAPL', 'DHR'], '52wkHigh': [60.79, 2050.49, 233.47, 110.11]})\n",
        "combined = pd.merge(openprice, wkhigh, how=\"left\", left_on='Symbol', right_on='Symbol')\n",
        "combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "udRlKnx-mULO"
      },
      "outputs": [],
      "source": [
        "# A. Join the stockname dataframe to our combined result and print the result\n",
        "stockname = pd.DataFrame({'Symbol': ['AMZN', 'DHR', 'DAL', 'AAPL'], 'Name': ['Amazon', 'Danaher', 'Delta Airlines', 'Apple']})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w70Gn3pmULP"
      },
      "outputs": [],
      "source": [
        "# B. Use the following tables from Super Store\n",
        "products = pd.read_csv('./datasets/products.csv')\n",
        "orders = pd.read_csv('./datasets/orders.csv')\n",
        "returns = pd.read_csv('./datasets/returns.csv')\n",
        "regions = pd.read_csv('./datasets/regions.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "q7eOWIKfmULP"
      },
      "outputs": [],
      "source": [
        "# i. We want to join the products and orders dataframes. \n",
        "#    Explore both dataframes to identify the common column between them\n",
        "#    Use a left join to combine the tables in a dataframe named orders_with_products\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIXexf8LmULQ"
      },
      "outputs": [],
      "source": [
        "# ii. Left join the orders_with_products and returns dataframes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdQs9E9VmULQ"
      },
      "outputs": [],
      "source": [
        "# iii. Finally, add the region data to our combined dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlMxeI5vmULR"
      },
      "outputs": [],
      "source": [
        "# iv. Let's use this combined dataframe to determine the salesperson generating the most profit\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "13. Cleaning and Combining Data.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}